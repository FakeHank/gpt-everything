# 自然语言模型简要发展历程

![Untitled](%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%AE%80%E8%A6%81%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B%20a62b09486e924b899b74fecf959706b4/Untitled.png)

- 统计语言模型：以统计词频为主要手段的 n-gram模型，只能建立短程依赖，非常受到数据集的影响
- 2010 年，RNN（Recurrent Neural Network）
    - 解决了 n-gram 模型捕捉句子中长期依赖能力有限的问题
    - 但依然存在长距离依赖的问题，而且是序列结构，无法进行并行计算
- 2013 年，word2vec：将每个词变成一个向量
    - 解决词语相似度的问题
    - 但每个词对应一个向量，无法解决一词多义的语境等问题
- **2017 年，Transformer：是当下 GPT 盛世的转折点**
    - 相关新闻：2018年10月，Google 发出一篇论文《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》，BERT 模型横空出世，并横扫 NLP 领域 11 项任务的最佳成绩！
    - 改变了以往序列建模和 RNN 划等号的思路，整个结构完全由注意力机制和前馈神经网络构成，架构如下
        
        ![Untitled](Untitled%202.png)
        
- 2018 年，GPT：GPT 系列的起点，利用了 Transformer 的解码器部分作为特征提取
    - 可以理解是让机器做文本续写
- 2018 年，BERT：利用了 Transformer 的编码器部分，对训练集进行双向训练
    - 可以理解是让机器做完形填空